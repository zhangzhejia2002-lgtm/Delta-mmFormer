{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b5K__YeXgzu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_single_fold(fold_idx, dataloaders):\n",
        "    \"\"\"Train a single fold\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold_idx + 1}/4\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Get current fold data\n",
        "    train_loader, val_loader = dataloaders[fold_idx]\n",
        "\n",
        "    # Initialize model\n",
        "    model = MMFormer_LiFS_Improved_v2(num_classes=2, dropout_rate=0.1).to(device)\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_epochs = 100\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 1e-3\n",
        "    patience = 30\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs,\n",
        "        eta_min=1e-6,\n",
        "        last_epoch=-1\n",
        "    )\n",
        "\n",
        "    # Gradient clipping\n",
        "    max_grad_norm = 1.0\n",
        "\n",
        "    # Early stopping state\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # Training history\n",
        "    train_history = {'loss': [], 'acc': []}\n",
        "    val_history = {'loss': [], 'acc': []}\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    ckpt_dir = f'./checkpoints/fold_{fold_idx}'\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        # ---- Training phase ----\n",
        "        model.train()\n",
        "        train_bar = tqdm(train_loader, desc=f\"[Fold {fold_idx+1}] Epoch {epoch:03d}\", leave=False)\n",
        "        train_losses, train_preds, train_labels = [], [], []\n",
        "\n",
        "        for batch in train_bar:\n",
        "            imgs = batch['image'].to(device)\n",
        "            masks = batch['modality_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(imgs, masks)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Record metrics\n",
        "            train_losses.append(loss.item())\n",
        "            preds = logits.argmax(dim=1).cpu().tolist()\n",
        "            train_preds += preds\n",
        "            train_labels += labels.cpu().tolist()\n",
        "\n",
        "            # Update progress bar\n",
        "            current_acc = accuracy_score(train_labels, train_preds)\n",
        "            train_bar.set_postfix(\n",
        "                loss=f\"{loss.item():.4f}\",\n",
        "                acc=f\"{current_acc:.4f}\"\n",
        "            )\n",
        "\n",
        "        # Training statistics\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "        train_history['loss'].append(avg_train_loss)\n",
        "        train_history['acc'].append(train_acc)\n",
        "\n",
        "        # ---- Validation phase ----\n",
        "        model.eval()\n",
        "        val_bar = tqdm(val_loader, desc=f\"[Fold {fold_idx+1}] Validating\", leave=False)\n",
        "        val_losses, val_preds, val_labels = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_bar:\n",
        "                imgs = batch['image'].to(device)\n",
        "                masks = batch['modality_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                logits = model(imgs, masks)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                val_losses.append(loss.item())\n",
        "                preds = logits.argmax(dim=1).cpu().tolist()\n",
        "                val_preds += preds\n",
        "                val_labels += labels.cpu().tolist()\n",
        "\n",
        "        # Validation statistics\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "        val_history['loss'].append(avg_val_loss)\n",
        "        val_history['acc'].append(val_acc)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"[Fold {fold_idx+1}] Epoch {epoch:03d} | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step()\n",
        "\n",
        "        # Early stopping & save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "\n",
        "            # Save best model checkpoint\n",
        "            ckpt_path = os.path.join(ckpt_dir, f\"best_epoch_{epoch:03d}.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'fold': fold_idx,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'train_history': train_history,\n",
        "                'val_history': val_history\n",
        "            }, ckpt_path)\n",
        "\n",
        "            print(f\"[Fold {fold_idx+1}] Saved checkpoint (Val Acc: {best_val_acc:.4f}) -> {ckpt_path}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"[Fold {fold_idx+1}] Early stopping at epoch {epoch} (no improvement for {patience} epochs)\")\n",
        "                break\n",
        "\n",
        "    print(f\"[Fold {fold_idx+1}] Training completed | Best Val Acc: {best_val_acc:.4f}\\n\")\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "def run_4fold_training(dataloaders):\n",
        "    \"\"\"Run 4-fold cross-validation training\"\"\"\n",
        "\n",
        "    # 4-fold cross-validation loop\n",
        "    fold_results = []\n",
        "\n",
        "    print(\"\\nStarting 4-Fold Cross-Validation Training...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for fold_idx in range(4):\n",
        "        best_acc = train_single_fold(fold_idx, dataloaders)\n",
        "        fold_results.append(best_acc)\n",
        "\n",
        "    # Compute cross-validation statistics\n",
        "    mean_acc = np.mean(fold_results)\n",
        "    std_acc = np.std(fold_results)\n",
        "\n",
        "    print(\"4-Fold Cross-Validation Results\")\n",
        "    for i, acc in enumerate(fold_results):\n",
        "        print(f\"Fold {i+1}: {acc:.4f}\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"Mean Accuracy: {mean_acc:.4f} +/- {std_acc:.4f}\")\n",
        "    print(f\"Best Fold: {max(fold_results):.4f}\")\n",
        "    print(f\"Worst Fold: {min(fold_results):.4f}\")\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# Execute training\n",
        "fold_results = run_4fold_training(dataloaders)"
      ]
    }
  ]
}