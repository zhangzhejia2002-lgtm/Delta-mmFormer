{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-pRzJpDCkas"
      },
      "outputs": [],
      "source": [
        "!pip install numpy>=2.0.0\n",
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    RandFlipd,\n",
        "    RandRotate90d,\n",
        "    RandRotateD,\n",
        "    RandZoomd,\n",
        "    RandScaleIntensityd,\n",
        "    RandGaussianNoised,\n",
        "    RandAdjustContrastd,\n",
        "    RandGaussianSmoothd,\n",
        "    RandShiftIntensityd,\n",
        "    ToTensord,\n",
        ")\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/LiQA_training_data/npz_sliced\"\n",
        "batch_size = 8\n",
        "num_workers = 2\n",
        "n_splits = 4\n",
        "seed = 42\n",
        "\n",
        "# Helper Functions\n",
        "def get_stage_from_path(path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract stage name (S1–S4) from a file path.\n",
        "\n",
        "    Expected: the path contains one of 'S1', 'S2', 'S3', 'S4'.\n",
        "    \"\"\"\n",
        "    parts = path.split(os.sep)\n",
        "    for p in parts:\n",
        "        if p in (\"S1\", \"S2\", \"S3\", \"S4\"):\n",
        "            return p\n",
        "    raise ValueError(f\"Stage not found in path: {path}\")\n",
        "\n",
        "\n",
        "def stage_to_label(stage: str) -> int:\n",
        "    \"\"\"\n",
        "    Map stage to a binary label.\n",
        "    - S1/S2/S3 -> 0\n",
        "    - S4       -> 1\n",
        "    \"\"\"\n",
        "    return 0 if stage in (\"S1\", \"S2\", \"S3\") else 1\n",
        "\n",
        "# Build File Index (path / stage / label)\n",
        "file_info = []\n",
        "for root, _, files in os.walk(data_dir):\n",
        "    for fname in files:\n",
        "        if fname.endswith(\".npz\"):\n",
        "            path = os.path.join(root, fname)\n",
        "            stage = get_stage_from_path(path)\n",
        "            label = stage_to_label(stage)\n",
        "            file_info.append({\"path\": path, \"stage\": stage, \"label\": label})\n",
        "\n",
        "print(f\"Found {len(file_info)} npz files in total.\")\n",
        "print(\"Stage distribution:\", Counter([f[\"stage\"] for f in file_info]))\n",
        "print(\"Class distribution:\", Counter([f[\"label\"] for f in file_info]))\n",
        "\n",
        "# Stage-Aware Augmentation\n",
        "stage_transforms = {\n",
        "    # S1: most samples -> mild augmentation\n",
        "    \"S1\": Compose(\n",
        "        [\n",
        "            RandFlipd(keys=[\"image\"], prob=0.3, spatial_axis=0),\n",
        "            RandRotate90d(keys=[\"image\"], prob=0.2, max_k=3),\n",
        "            ToTensord(keys=[\"image\"]),\n",
        "        ]\n",
        "    ),\n",
        "    # S2: fewer samples -> moderate augmentation\n",
        "    \"S2\": Compose(\n",
        "        [\n",
        "            RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=(0, 1)),\n",
        "            RandRotate90d(keys=[\"image\"], prob=0.4, max_k=3),\n",
        "            RandRotateD(keys=[\"image\"], range_x=0.1745, prob=0.4),\n",
        "            RandZoomd(keys=[\"image\"], min_zoom=0.95, max_zoom=1.05, prob=0.4),\n",
        "            RandScaleIntensityd(keys=[\"image\"], factors=0.1, prob=0.3),\n",
        "            ToTensord(keys=[\"image\"]),\n",
        "        ]\n",
        "    ),\n",
        "    # S3: least samples -> strong augmentation (key design)\n",
        "    \"S3\": Compose(\n",
        "        [\n",
        "            RandFlipd(keys=[\"image\"], prob=0.8, spatial_axis=(0, 1, 2)),\n",
        "            RandRotate90d(keys=[\"image\"], prob=0.7, max_k=3),\n",
        "            RandRotateD(keys=[\"image\"], range_x=0.5236, prob=0.6),\n",
        "            RandZoomd(keys=[\"image\"], min_zoom=0.8, max_zoom=1.2, prob=0.6),\n",
        "            RandScaleIntensityd(keys=[\"image\"], factors=0.2, prob=0.5),\n",
        "            RandGaussianNoised(keys=[\"image\"], prob=0.4, std=0.02),\n",
        "            RandAdjustContrastd(keys=[\"image\"], prob=0.4, gamma=(0.7, 1.3)),\n",
        "            RandGaussianSmoothd(keys=[\"image\"], prob=0.3, sigma_x=(0.5, 1.0)),\n",
        "            RandShiftIntensityd(keys=[\"image\"], prob=0.3, offsets=0.1),\n",
        "            ToTensord(keys=[\"image\"]),\n",
        "        ]\n",
        "    ),\n",
        "    # S4: moderate amount -> medium augmentation\n",
        "    \"S4\": Compose(\n",
        "        [\n",
        "            RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=(0, 1)),\n",
        "            RandRotate90d(keys=[\"image\"], prob=0.4, max_k=3),\n",
        "            RandRotateD(keys=[\"image\"], range_x=0.1745, prob=0.4),\n",
        "            RandZoomd(keys=[\"image\"], min_zoom=0.95, max_zoom=1.05, prob=0.4),\n",
        "            RandScaleIntensityd(keys=[\"image\"], factors=0.1, prob=0.3),\n",
        "            ToTensord(keys=[\"image\"]),\n",
        "        ]\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Validation: no random augmentation, tensor conversion only\n",
        "val_transform = Compose([ToTensord(keys=[\"image\"])])\n",
        "\n",
        "# Dataset Definition\n",
        "class MRIDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Load one .npz sample and apply:\n",
        "    - stage-aware random augmentation for training\n",
        "    - minimal transform for validation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, infos, train: bool = True):\n",
        "        self.infos = infos\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.infos)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        info = self.infos[idx]\n",
        "\n",
        "        # Load data from npz\n",
        "        data = np.load(info[\"path\"])\n",
        "        img = data[\"image\"].astype(np.float32)\n",
        "        mask = data[\"mask\"].astype(np.int8)\n",
        "\n",
        "        stage = info[\"stage\"]\n",
        "        label = info[\"label\"]\n",
        "\n",
        "        # Apply transform (Monai dict-style transforms)\n",
        "        sample = {\"image\": img}\n",
        "        if self.train:\n",
        "            sample = stage_transforms[stage](sample)\n",
        "        else:\n",
        "            sample = val_transform(sample)\n",
        "\n",
        "        # Return tensors used by the model\n",
        "        return {\n",
        "            \"image\": sample[\"image\"],\n",
        "            \"modality_mask\": torch.from_numpy(mask),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Stratified K-Fold Split + DataLoaders (Stratify by stage to keep S1–S4 distribution similar across folds)\n",
        "stage_labels = [int(info[\"stage\"][1]) for info in file_info]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "dataloaders = []\n",
        "\n",
        "for fold, (tr_idx, vl_idx) in enumerate(skf.split(file_info, stage_labels), 1):\n",
        "    tr_infos = [file_info[i] for i in tr_idx]\n",
        "    vl_infos = [file_info[i] for i in vl_idx]\n",
        "\n",
        "    tr_ds = MRIDataset(tr_infos, train=True)\n",
        "    vl_ds = MRIDataset(vl_infos, train=False)\n",
        "\n",
        "    tr_loader = DataLoader(\n",
        "        tr_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    vl_loader = DataLoader(\n",
        "        vl_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    dataloaders.append((tr_loader, vl_loader))\n",
        "\n",
        "    print(\n",
        "        f\"Fold {fold}: train={len(tr_ds)}, val={len(vl_ds)}\",\n",
        "        \"| Stage_train=\", Counter([i[\"stage\"] for i in tr_infos]),\n",
        "        \"Stage_val=\", Counter([i[\"stage\"] for i in vl_infos]),)\n",
        "\n",
        "print(f\"\\nPipeline ready. Total folds: {len(dataloaders)}\")\n"
      ],
      "metadata": {
        "id": "lRz37fxyC3iF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}