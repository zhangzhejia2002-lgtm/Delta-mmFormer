{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc0y28hKOjNW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\"\"\"\n",
        "MMFormer_LiFS_Improved_v2\n",
        "\n",
        "This file implements an improved MMFormer for Liver Fibrosis Staging (LiFS),\n",
        "with missing-modality compensation via a Delta Function, following:\n",
        "- Modality-specific encoder (Sec. 2.3)\n",
        "- Intra-modality Transformer (Sec. 2.3, Eq. (4)-(8))\n",
        "- Delta Function for missing-modality compensation (Sec. 2.4, Eq. (9)-(13))\n",
        "- Modality-correlated encoder / cross-modality Transformer fusion (Sec. 2.5, Eq. (14)-(17))\n",
        "- Classification head (Sec. 2.6)\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Global configuration\n",
        "# =========================\n",
        "# Paper-aligned typical settings:\n",
        "# C_base = 8, C_t = 256, h = 4 heads, L = 4 Transformer blocks, M = 3 modalities\n",
        "basic_dims = 8\n",
        "transformer_basic_dims = 256\n",
        "mlp_dim = 1024\n",
        "num_heads = 4\n",
        "depth = 4\n",
        "num_modals = 3\n",
        "num_classes = 2\n",
        "fusion_dim = transformer_basic_dims\n",
        "\n",
        "# =========================\n",
        "# Normalization helper\n",
        "# =========================\n",
        "def normalization(planes, norm='bn'):\n",
        "    \"\"\"\n",
        "    Normalization options.\n",
        "    The paper typically uses InstanceNorm3d (IN) for 3D conv units in the encoder (Sec. 2.3).\n",
        "    \"\"\"\n",
        "    if norm == 'bn':\n",
        "        return nn.BatchNorm3d(planes)\n",
        "    elif norm == 'gn':\n",
        "        return nn.GroupNorm(4, planes)\n",
        "    elif norm == 'in':\n",
        "        return nn.InstanceNorm3d(planes, affine=True)\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported norm: {norm}')\n",
        "\n",
        "# =========================\n",
        "# 3D Conv Unit used in the encoder\n",
        "# =========================\n",
        "class general_conv3d_prenorm(nn.Module):\n",
        "    \"\"\"\n",
        "    3D Conv -> Norm -> Activation -> (optional) Dropout3d\n",
        "\n",
        "    Paper correspondence:\n",
        "    - Residual 3D conv unit: IN + LeakyReLU (+ dropout) (Sec. 2.3)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, k_size=3, stride=1, padding=1, pad_type='zeros',\n",
        "                 norm='in', act_type='lrelu', relufactor=0.2, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv3d(in_ch, out_ch, k_size, stride, padding, padding_mode=pad_type, bias=True)\n",
        "        self.norm = normalization(out_ch, norm)\n",
        "        self.act = nn.LeakyReLU(negative_slope=relufactor, inplace=True) if act_type == 'lrelu' else nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout3d(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.act(self.norm(self.conv(x))))\n",
        "\n",
        "# =========================\n",
        "# Modality-specific Encoder (5-stage 3D Res-CNN)\n",
        "# =========================\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    5-stage residual 3D convolutional encoder.\n",
        "\n",
        "    Paper correspondence (Sec. 2.3):\n",
        "    - Stages s=1..5\n",
        "      * stage1: stride=1\n",
        "      * stage2-5: stride=2 down-sampling\n",
        "    - Channel progression roughly: C_s = 2^(s-1) * C_base  (Eq. (1) style description)\n",
        "      * s1: 8, s2: 16, s3: 32, s4: 64, s5: 128  (given C_base=8)\n",
        "    - Residual within each stage: x_s = x_s + Conv(Conv(x_s))  (Eq. (2) style)\n",
        "    - Total down-sampling: 2^4 = 16 => output resolution ~ (D,H,W)/16\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        # Stage 1 (no down-sampling)\n",
        "        self.e1_c1 = nn.Conv3d(1, basic_dims, 3, 1, 1, padding_mode='reflect', bias=True)\n",
        "        self.e1_c2 = general_conv3d_prenorm(basic_dims, basic_dims, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e1_c3 = general_conv3d_prenorm(basic_dims, basic_dims, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "\n",
        "        # Stage 2 (down-sampling by stride=2)\n",
        "        self.e2_c1 = general_conv3d_prenorm(basic_dims, basic_dims * 2, stride=2, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e2_c2 = general_conv3d_prenorm(basic_dims * 2, basic_dims * 2, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e2_c3 = general_conv3d_prenorm(basic_dims * 2, basic_dims * 2, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "\n",
        "        # Stage 3\n",
        "        self.e3_c1 = general_conv3d_prenorm(basic_dims * 2, basic_dims * 4, stride=2, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e3_c2 = general_conv3d_prenorm(basic_dims * 4, basic_dims * 4, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e3_c3 = general_conv3d_prenorm(basic_dims * 4, basic_dims * 4, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "\n",
        "        # Stage 4\n",
        "        self.e4_c1 = general_conv3d_prenorm(basic_dims * 4, basic_dims * 8, stride=2, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e4_c2 = general_conv3d_prenorm(basic_dims * 8, basic_dims * 8, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e4_c3 = general_conv3d_prenorm(basic_dims * 8, basic_dims * 8, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "\n",
        "        # Stage 5\n",
        "        self.e5_c1 = general_conv3d_prenorm(basic_dims * 8, basic_dims * 16, stride=2, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e5_c2 = general_conv3d_prenorm(basic_dims * 16, basic_dims * 16, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "        self.e5_c3 = general_conv3d_prenorm(basic_dims * 16, basic_dims * 16, pad_type='reflect', dropout_rate=dropout_rate)\n",
        "\n",
        "        # Additional regularization\n",
        "        self.final_dropout = nn.Dropout3d(dropout_rate * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stage 1 residual\n",
        "        x1 = self.e1_c1(x)\n",
        "        x1 = x1 + self.e1_c3(self.e1_c2(x1))\n",
        "\n",
        "        # Stage 2 residual\n",
        "        x2 = self.e2_c1(x1)\n",
        "        x2 = x2 + self.e2_c3(self.e2_c2(x2))\n",
        "\n",
        "        # Stage 3 residual\n",
        "        x3 = self.e3_c1(x2)\n",
        "        x3 = x3 + self.e3_c3(self.e3_c2(x3))\n",
        "\n",
        "        # Stage 4 residual\n",
        "        x4 = self.e4_c1(x3)\n",
        "        x4 = x4 + self.e4_c3(self.e4_c2(x4))\n",
        "\n",
        "        # Stage 5 residual\n",
        "        x5 = self.e5_c1(x4)\n",
        "        x5 = x5 + self.e5_c3(self.e5_c2(x5))\n",
        "\n",
        "        return self.final_dropout(x5)\n",
        "\n",
        "# =========================\n",
        "# Self-Attention + Transformer Blocks\n",
        "# =========================\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self-attention (MSA).\n",
        "\n",
        "    Paper correspondence (Sec. 2.3 / 2.5):\n",
        "    - QKV projection, scaled dot-product attention, dropout on attention/projection\n",
        "    - Used in both intra-modality transformer and cross-modality transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, heads=8, qkv_bias=False, qk_scale=None, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.num_heads = heads\n",
        "        head_dim = dim // heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(dropout_rate)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj_drop(self.proj(x))\n",
        "        return x\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    \"\"\"Standard residual wrapper: y = f(x) + x (matches residual forms in Eq. (7)(8)(16)(17)).\"\"\"\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(x) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    \"\"\"Pre-LN: y = f(LN(x)) (paper uses LN around attention/FFN blocks).\"\"\"\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(self.norm(x))\n",
        "\n",
        "class PreNormDrop(nn.Module):\n",
        "    \"\"\"Pre-LN + dropout wrapper (implementation detail for regularization).\"\"\"\n",
        "    def __init__(self, dim, dropout_rate, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.drop = nn.Dropout(dropout_rate)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.drop(self.fn(self.norm(x)))\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.gelu(x)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    FFN / MLP block: Linear -> GELU -> Dropout -> Linear -> Dropout\n",
        "    Paper correspondence: FFN described in transformer equations (Sec. 2.3/2.5).\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, hidden_dim, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder stack (used for both intra-modality and cross-modality encoding).\n",
        "\n",
        "    Paper correspondence:\n",
        "    - Add sinusoidal positional encoding at each layer: T^l = T^{l-1} + P  (Eq. (4) / Eq. (15))\n",
        "    - Then MSA + residual + LN, and FFN + residual + LN (Eq. (5)-(8), Eq. (16)-(17))\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, depth, heads, mlp_dim, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.cross_attn = nn.ModuleList([\n",
        "            Residual(PreNormDrop(embedding_dim, dropout_rate, SelfAttention(embedding_dim, heads, dropout_rate=dropout_rate)))\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.cross_ffn = nn.ModuleList([\n",
        "            Residual(PreNorm(embedding_dim, FeedForward(embedding_dim, mlp_dim, dropout_rate)))\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.depth = depth\n",
        "\n",
        "    def forward(self, x, pos):\n",
        "        for i in range(self.depth):\n",
        "            # Eq. (4) / Eq. (15): add sinusoidal PE at every transformer layer\n",
        "            x = x + pos\n",
        "            # Eq. (5)-(8) or Eq. (16)-(17): MSA + FFN with residual structure\n",
        "            x = self.cross_attn[i](x)\n",
        "            x = self.cross_ffn[i](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Delta Function: Missing Modality Compensation\n",
        "# =========================\n",
        "class DeltaFunction(nn.Module):\n",
        "    \"\"\"\n",
        "    Delta Function for missing-modality compensation.\n",
        "\n",
        "    Paper correspondence (Sec. 2.4):\n",
        "    - Learnable modality calibration parameters (Eq. (9)):\n",
        "        T̂_m = ((T_m - μ_m) / (σ_m + ε)) ⊙ w_m\n",
        "      where μ_m, σ_m, w_m are learnable per modality.\n",
        "    - Reference token computed by averaging available modalities (Eq. (10))\n",
        "    - Proxy feature generation via MSA on reference tokens (Eq. (11))\n",
        "    - Proxy refinement and attenuation α to down-weight inferred features (Eq. (12)-(13))\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim, num_modals=3, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.num_modals = num_modals\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Learnable calibration parameters: μ_m, σ_m, w_m (Eq. (9))\n",
        "        self.delta_mean = nn.Parameter(torch.zeros(num_modals, feature_dim))\n",
        "        self.delta_std = nn.Parameter(torch.ones(num_modals, feature_dim))\n",
        "        self.delta_weight = nn.Parameter(torch.ones(num_modals, feature_dim))\n",
        "\n",
        "        # Cross-modal attention used to synthesize proxy features (Eq. (11))\n",
        "        self.cross_modal_attention = nn.MultiheadAttention(\n",
        "            feature_dim, num_heads=4, dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Compensation network (FFN refinement): Eq. (12)-(13) style refinement\n",
        "        self.compensation_net = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(feature_dim, feature_dim),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, modal_tokens, missing_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            modal_tokens: list length=num_modals, each is (B, N, C) or None\n",
        "            missing_mask: (B, num_modals), 1=present, 0=missing\n",
        "\n",
        "        Returns:\n",
        "            compensated: list length=num_modals, each (B, N, C) with missing ones replaced by proxy\n",
        "        \"\"\"\n",
        "        batch_size = missing_mask.size(0)\n",
        "        device = missing_mask.device\n",
        "        compensated = []\n",
        "        available_features = []\n",
        "\n",
        "        # ---- Step A: calibrate available modalities (Eq. (9)) ----\n",
        "        for i, tokens in enumerate(modal_tokens):\n",
        "            if tokens is not None and missing_mask[:, i].sum() > 0:\n",
        "                dm = self.delta_mean[i].view(1, 1, -1)\n",
        "                ds = self.delta_std[i].view(1, 1, -1)\n",
        "                dw = self.delta_weight[i].view(1, 1, -1)\n",
        "\n",
        "                # Eq. (9): Delta calibration\n",
        "                calib = (tokens - dm) / (ds + 1e-8) * dw\n",
        "\n",
        "                # apply availability mask per sample\n",
        "                mask_i = missing_mask[:, i].view(-1, 1, 1).expand_as(calib)\n",
        "                calib = calib * mask_i\n",
        "\n",
        "                compensated.append(calib)\n",
        "                available_features.append(calib)\n",
        "            else:\n",
        "                compensated.append(None)\n",
        "\n",
        "        # ---- Step B: synthesize proxy for missing modalities ----\n",
        "        if available_features:\n",
        "            # Eq. (10): reference token (average of available modalities)\n",
        "            ref_feature = torch.stack(available_features, dim=0).mean(dim=0)\n",
        "\n",
        "            for i, tokens in enumerate(compensated):\n",
        "                if tokens is None:\n",
        "                    # Eq. (11): proxy generation via attention on reference tokens\n",
        "                    proxy, _ = self.cross_modal_attention(ref_feature, ref_feature, ref_feature)\n",
        "\n",
        "                    # Eq. (12): modality-specific delta calibration on proxy (same form as Eq. (9))\n",
        "                    dm = self.delta_mean[i].view(1, 1, -1)\n",
        "                    ds = self.delta_std[i].view(1, 1, -1)\n",
        "                    dw = self.delta_weight[i].view(1, 1, -1)\n",
        "                    proxy = (proxy - dm) / (ds + 1e-8) * dw\n",
        "\n",
        "                    # Eq. (13): refinement + attenuation alpha (down-weight inferred features)\n",
        "                    proxy = self.compensation_net(proxy) * 0.3  # alpha = 0.3\n",
        "                    compensated[i] = proxy\n",
        "\n",
        "        return compensated\n",
        "\n",
        "# =========================\n",
        "# Main Model: MMFormer_LiFS_Improved_v2\n",
        "# =========================\n",
        "class MMFormer_LiFS_Improved_v2(nn.Module):\n",
        "    \"\"\"\n",
        "    Full model:\n",
        "    - Modality-specific encoders + projection (Sec. 2.3)\n",
        "    - Intra-modality transformer per modality (Sec. 2.3)\n",
        "    - Delta compensation for missing modalities (Sec. 2.4)\n",
        "    - Cross-modality transformer fusion (Sec. 2.5)\n",
        "    - Classification head (Sec. 2.6)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=2, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Modality-specific 3D Res-CNN encoders (Sec. 2.3)\n",
        "        self.encoders = nn.ModuleDict({\n",
        "            't1': Encoder(dropout_rate=dropout_rate),\n",
        "            't2': Encoder(dropout_rate=dropout_rate),\n",
        "            'dwi': Encoder(dropout_rate=dropout_rate)\n",
        "        })\n",
        "\n",
        "        # 1x1x1 conv projection to token embedding dim C_t=256 (Sec. 2.3)\n",
        "        self.encode_proj = nn.ModuleDict({\n",
        "            't1': nn.Sequential(nn.Conv3d(basic_dims * 16, transformer_basic_dims, 1), nn.Dropout3d(dropout_rate)),\n",
        "            't2': nn.Sequential(nn.Conv3d(basic_dims * 16, transformer_basic_dims, 1), nn.Dropout3d(dropout_rate)),\n",
        "            'dwi': nn.Sequential(nn.Conv3d(basic_dims * 16, transformer_basic_dims, 1), nn.Dropout3d(dropout_rate))\n",
        "        })\n",
        "\n",
        "        # Intra-modality transformers (Sec. 2.3, Eq. (4)-(8))\n",
        "        self.intra_transformers = nn.ModuleDict({\n",
        "            't1': Transformer(transformer_basic_dims, depth, num_heads, mlp_dim, dropout_rate),\n",
        "            't2': Transformer(transformer_basic_dims, depth, num_heads, mlp_dim, dropout_rate),\n",
        "            'dwi': Transformer(transformer_basic_dims, depth, num_heads, mlp_dim, dropout_rate)\n",
        "        })\n",
        "\n",
        "        # Pre/post LN around fusion tokens (Sec. 2.5 / 2.6)\n",
        "        self.norm_before = nn.LayerNorm(transformer_basic_dims)\n",
        "        self.norm_after = nn.LayerNorm(transformer_basic_dims)\n",
        "\n",
        "        # Delta Function (Sec. 2.4, Eq. (9)-(13))\n",
        "        self.delta_function = DeltaFunction(transformer_basic_dims, num_modals, dropout_rate)\n",
        "\n",
        "        # Cross-modality transformer fusion (Sec. 2.5, Eq. (14)-(17))\n",
        "        self.multimodal_transformer = Transformer(transformer_basic_dims, depth, num_heads, mlp_dim, dropout_rate)\n",
        "\n",
        "        # Fusion projection (Sec. 2.6)\n",
        "        self.fuse_proj = nn.Sequential(\n",
        "            nn.Linear(transformer_basic_dims, fusion_dim),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Classification head (Sec. 2.6)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, fusion_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(fusion_dim),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(fusion_dim // 2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(fusion_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initialization notes:\n",
        "        - Convs: Kaiming (leaky_relu)\n",
        "        - Linears: Xavier with smaller gain\n",
        "        This is an implementation choice for stability; does not change the model definition.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=0.8)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, (nn.BatchNorm3d, nn.LayerNorm, nn.BatchNorm1d)):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _create_pos_encoding(self, N, dim, device):\n",
        "        \"\"\"\n",
        "        Sinusoidal positional encoding.\n",
        "        Paper correspondence:\n",
        "        - Positional encoding P added at each transformer layer (Eq. (4) and Eq. (15)).\n",
        "        \"\"\"\n",
        "        pe = torch.zeros(1, N, dim, device=device)\n",
        "        pos = torch.arange(N, device=device).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, dim, 2, device=device).float() * -(math.log(10000.0) / dim))\n",
        "\n",
        "        pe[0, :, 0::2] = torch.sin(pos * div)\n",
        "        if dim % 2 == 1:\n",
        "            pe[0, :, 1::2] = torch.cos(pos * div[:-1])\n",
        "        else:\n",
        "            pe[0, :, 1::2] = torch.cos(pos * div)\n",
        "\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x, missing_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (B, 3, D, H, W) input volumes for modalities [t1, t2, dwi]\n",
        "            missing_mask: (B, 3) modality availability mask, 1=present, 0=missing\n",
        "\n",
        "        Returns:\n",
        "            logits: (B, num_classes)\n",
        "\n",
        "        Paper correspondence:\n",
        "        - Sec. 2.3: modality-specific encoder -> tokenization -> intra-modality transformer\n",
        "        - Sec. 2.4: delta compensation for missing modalities\n",
        "        - Sec. 2.5: concatenate + cross-modality transformer fusion\n",
        "        - Sec. 2.6: LN + pooling + classifier\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        device = x.device\n",
        "        modality_names = ['t1', 't2', 'dwi']\n",
        "\n",
        "        # ===== Step 1: modality-specific encoding (Sec. 2.3) =====\n",
        "        modal_tokens = []\n",
        "\n",
        "        for i, modality in enumerate(modality_names):\n",
        "            # If at least one sample in this batch has this modality, compute tokens\n",
        "            if missing_mask[:, i].sum() > 0:\n",
        "                modal_input = x[:, i:i+1, :, :, :]\n",
        "\n",
        "                # 3D Res-CNN encoder output: (B, C, d, h, w)\n",
        "                modal_feat = self.encoders[modality](modal_input)\n",
        "\n",
        "                # 1x1x1 projection to token embedding dim C_t=256 (Sec. 2.3)\n",
        "                modal_token = self.encode_proj[modality](modal_feat)\n",
        "                B, C, D, H, W = modal_token.shape\n",
        "                modal_token = modal_token.view(B, C, -1).permute(0, 2, 1)  # (B, N, C)\n",
        "\n",
        "                # Intra-modality transformer w/ PE at each layer (Eq. (4)-(8))\n",
        "                num_tokens = modal_token.size(1)\n",
        "                pos_encoding = self._create_pos_encoding(num_tokens, transformer_basic_dims, device)\n",
        "                modal_token = self.intra_transformers[modality](modal_token, pos_encoding)\n",
        "\n",
        "                modal_tokens.append(modal_token)\n",
        "            else:\n",
        "                modal_tokens.append(None)\n",
        "\n",
        "        # ===== Step 2: Delta function compensation (Sec. 2.4, Eq. (9)-(13)) =====\n",
        "        compensated_tokens = self.delta_function(modal_tokens, missing_mask)\n",
        "\n",
        "        # ===== Step 3: gather valid tokens (present + proxy) =====\n",
        "        valid_tokens = [token for token in compensated_tokens if token is not None]\n",
        "\n",
        "        if len(valid_tokens) == 0:\n",
        "            # Extreme case: all modalities missing for the whole batch\n",
        "            return torch.zeros(batch_size, num_classes, device=device)\n",
        "\n",
        "        # Eq. (14): align token length by truncating to the shortest sequence\n",
        "        min_length = min(token.size(1) for token in valid_tokens)\n",
        "        aligned_tokens = [token[:, :min_length, :] for token in valid_tokens]\n",
        "\n",
        "        # Eq. (14): concatenate tokens across modalities along the token dimension\n",
        "        fused = torch.cat(aligned_tokens, dim=1)  # (B, total_N, C)\n",
        "\n",
        "        # ===== Step 4: cross-modality transformer fusion (Sec. 2.5, Eq. (15)-(17)) =====\n",
        "        fused = self.norm_before(fused)\n",
        "\n",
        "        fused_length = fused.size(1)\n",
        "        pos = self._create_pos_encoding(fused_length, transformer_basic_dims, device)\n",
        "\n",
        "        fused = self.multimodal_transformer(fused, pos)\n",
        "        fused = self.norm_after(fused)\n",
        "\n",
        "        # ===== Step 5: pooling + classification (Sec. 2.6) =====\n",
        "        pooled = fused.mean(dim=1) # global average pooling over tokens\n",
        "        fused_vec = self.fuse_proj(pooled)  # (B, fusion_dim)\n",
        "        logits = self.classifier(fused_vec) # (B, num_classes)\n",
        "\n",
        "        return logits\n"
      ]
    }
  ]
}